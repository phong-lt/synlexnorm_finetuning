EXECUTOR: "Transformer_Executor"

DEVICE: "cuda"
SEED: 0
SAVE: TRUE
SAVE_PATH: "synlexnorm_finetuning/models"

# Model
NumEncoderLayers: 3
NumDecoderLayers: 3
EmbedSize: 512
NHEAD: 8
FFW: 2048

# Tokenizer
Tokenizer: "BPE_Tokenizer"
bpe_step: 2000
vocab_save_path: "synlexnorm_finetuning/bpe_tokenizer.json"
max_vocab_size: 7000
bpe_path:
  - "/wiki20k.csv"
  - "/train.csv"

# Pretraining
DO_PRETRAINING: TRUE


#Pretrain-Train Hyper

## Batch size
PRETRAIN_BATCH_SIZE: 8
TRAIN_BATCH_SIZE: 8
EVAL_BATCH_SIZE: 16
PREDICT_BATCH_SIZE: 16

NUMWORKERS: 2
## Optim
LR: 0.0001
BETAS: 
  - 0.9
  - 0.98
warmup_step: 1000

## Steps
NUM_PRETRAIN_STEP: 10000
show_loss_after_pretrain_steps: 200
save_after_pretrain_steps: 2000

NUM_TRAIN_STEP: 10000
show_loss_after_steps: 200
eval_after_steps: 1000

max_eval_length: 128
## Data path
pretrain_data_path: "/wiki20k.csv"
train_path: "/train.csv"
val_path: "/dev.csv"
predict_path: "/dev.csv"

src_max_token_len: 256
trg_max_token_len: 256

## Predict
get_predict_score: TRUE
max_predict_length: 256

